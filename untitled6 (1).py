# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mz8a8MFfKX2XOdx0m87qB_ilnlmgxmNx
"""

#!/usr/bin/env python3
"""
Time series forecasting pipeline: synthetic multivariate data, Transformer (attention) model,
LSTM baseline, SARIMA baseline if available (otherwise seasonal-naive), training, evaluation,
and attention visualization.

Run: python time_series_forecasting_pipeline.py
"""

import os
import math
import random
from dataclasses import dataclass
from typing import Tuple, Optional, List

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import trange

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error

# optional
try:
    import statsmodels.api as sm
    STATSMODELS_AVAILABLE = True
except Exception:
    STATSMODELS_AVAILABLE = False

# reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", DEVICE)

# -----------------------
# 1) Synthetic data generator
# -----------------------
def generate_multivariate_time_series(n_steps=1800, n_features=6,
                                      seasonal_periods=(24, 168), trend_slope=0.0009, noise_std=0.15):
    """
    Generate multivariate series with multiple seasonalities, a small trend and feature coupling.
    Returns pandas.DataFrame with feature_0..feature_{k-1} and 'target'.
    """
    t = np.arange(n_steps)
    base = (2.0 * np.sin(2 * math.pi * t / seasonal_periods[0])
            + 1.5 * np.sin(2 * math.pi * t / seasonal_periods[1])
            + trend_slope * t)
    data = {}
    for i in range(n_features):
        amp = 0.4 + 0.15 * i
        phase = (i * 0.25) % (2 * math.pi)
        feat = amp * np.sin(2 * math.pi * t / seasonal_periods[0] + phase)
        feat += 0.25 * np.cos(2 * math.pi * t / seasonal_periods[1] * (0.9 + 0.05 * i))
        feat += 0.08 * np.roll(base, (i + 1) * 2)
        feat += noise_std * np.random.randn(n_steps)
        data[f"feature_{i}"] = feat
    coefs = np.linspace(0.2, 1.0, n_features)
    feat_mat = np.vstack([data[f"feature_{i}"] for i in range(n_features)])
    target = base + (coefs @ feat_mat) / n_features + noise_std * np.random.randn(n_steps)
    data["target"] = target
    return pd.DataFrame(data)

# -----------------------
# 2) Prepare datasets (train/val/test), scaling and PyTorch Dataset
# -----------------------
@dataclass
class SeqConfig:
    input_len: int = 96
    output_len: int = 24
    test_size: int = 300  # last N points reserved for test

cfg = SeqConfig()

class SlidingWindowDataset(Dataset):
    def __init__(self, arr: np.ndarray, input_len: int, output_len: int):
        """
        arr shape: (n_time, n_channels) where last column is target
        returns X: (input_len, n_features), y: (output_len,)
        """
        self.arr = arr.astype(np.float32)
        self.input_len = input_len
        self.output_len = output_len
        self.n = arr.shape[0]
        self.n_samples = max(0, self.n - (input_len + output_len) + 1)
    def __len__(self):
        return self.n_samples
    def __getitem__(self, idx):
        start = idx
        X = self.arr[start:start + self.input_len, :-1]  # features
        y = self.arr[start + self.input_len:start + self.input_len + self.output_len, -1]
        return X, y

def split_scale(df: pd.DataFrame, cfg: SeqConfig):
    n = len(df)
    test_n = cfg.test_size
    train_val_end = n - test_n
    val_n = max(1, int(0.1 * train_val_end))
    train_n = train_val_end - val_n
    df_train = df.iloc[:train_n].copy()
    df_val = df.iloc[train_n:train_val_end].copy()
    df_test = df.iloc[train_val_end:].copy()
    feat_cols = [c for c in df.columns if c != "target"]
    scaler_x = StandardScaler().fit(df_train[feat_cols])
    scaler_y = StandardScaler().fit(df_train[["target"]])
    def to_arr(df_):
        X = scaler_x.transform(df_[feat_cols])
        y = scaler_y.transform(df_[["target"]]).flatten()
        return np.hstack([X, y.reshape(-1, 1)])
    return to_arr(df_train), to_arr(df_val), to_arr(df_test), scaler_x, scaler_y, df_train, df_val, df_test

# -----------------------
# 3) Model definitions
#    - Custom Transformer encoder stack that returns attention weights per layer (averaged across heads)
#    - LSTM baseline
# -----------------------
class TransformerEncoderBlock(nn.Module):
    def __init__(self, d_model, n_heads, dim_feedforward=256, dropout=0.1):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)
        self.norm1 = nn.LayerNorm(d_model)
        self.ff = nn.Sequential(
            nn.Linear(d_model, dim_feedforward),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(dim_feedforward, d_model),
            nn.Dropout(dropout)
        )
        self.norm2 = nn.LayerNorm(d_model)
    def forward(self, x):
        # x: (b, seq, d_model)
        attn_out, attn_weights = self.self_attn(x, x, x, need_weights=True)  # attn_weights shape (b, seq, seq) averaged across heads
        x = self.norm1(x + attn_out)
        ff_out = self.ff(x)
        x = self.norm2(x + ff_out)
        return x, attn_weights  # attn_weights: (batch, tgt_len, src_len)

class SimpleTransformerForecast(nn.Module):
    def __init__(self, input_dim, d_model=64, n_heads=4, n_layers=3, ff_dim=256,
                 input_len=96, output_len=24, dropout=0.1):
        super().__init__()
        self.input_len = input_len
        self.output_len = output_len
        self.input_proj = nn.Linear(input_dim, d_model)
        self.pos_emb = nn.Parameter(self._positional_encoding(input_len, d_model), requires_grad=False)
        self.layers = nn.ModuleList([TransformerEncoderBlock(d_model, n_heads, ff_dim, dropout) for _ in range(n_layers)])
        self.head = nn.Sequential(nn.Linear(d_model * input_len, 256), nn.ReLU(), nn.Linear(256, output_len))
    @staticmethod
    def _positional_encoding(seq_len, d_model):
        pe = torch.zeros(seq_len, d_model)
        pos = torch.arange(0, seq_len).unsqueeze(1).float()
        div = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(pos * div)
        pe[:, 1::2] = torch.cos(pos * div)
        return pe.unsqueeze(0)  # shape (1, seq_len, d_model)
    def forward(self, x, return_attns: bool = False):
        # x: (b, seq, input_dim)
        b = x.shape[0]
        x = self.input_proj(x)  # (b, seq, d_model)
        x = x + self.pos_emb.to(x.device)
        attns: List[np.ndarray] = []
        for layer in self.layers:
            x, attn_w = layer(x)  # attn_w (b, seq, seq)
            if return_attns:
                attns.append(attn_w)
        flattened = x.reshape(b, -1)
        out = self.head(flattened)
        if return_attns:
            # stack into (n_layers, batch, seq, seq)
            attns_tensor = torch.stack(attns, dim=0).detach().cpu().numpy()
            return out, attns_tensor
        return out

class LSTMForecast(nn.Module):
    def __init__(self, input_dim, hidden=128, num_layers=2, output_len=24, dropout=0.1):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden, num_layers=num_layers, batch_first=True, dropout=dropout)
        self.head = nn.Sequential(nn.Linear(hidden, 128), nn.ReLU(), nn.Linear(128, output_len))
    def forward(self, x):
        out, _ = self.lstm(x)
        last = out[:, -1, :]
        return self.head(last)

# -----------------------
# 4) Training utilities
# -----------------------
def train_epoch(model, loader, optimizer, criterion, device):
    model.train()
    total_loss = 0.0
    for X, y in loader:
        X = X.to(device); y = y.to(device)
        optimizer.zero_grad()
        preds = model(X)
        loss = criterion(preds, y)
        loss.backward()
        optimizer.step()
        total_loss += float(loss.item()) * X.size(0)
    return total_loss / max(1, len(loader.dataset))

def evaluate(model, loader, criterion, device, return_attn=False):
    model.eval()
    losses = 0.0
    preds_list, y_list = [], []
    attns_collect = []
    with torch.no_grad():
        for X, y in loader:
            X = X.to(device); y = y.to(device)
            if return_attn:
                p, att = model(X, return_attns=True)
                attns_collect.append(att)
            else:
                p = model(X)
            preds_list.append(p.cpu().numpy()); y_list.append(y.cpu().numpy())
    preds = np.vstack(preds_list) if preds_list else np.zeros((0, 1))
    ytrue = np.vstack(y_list) if y_list else np.zeros((0, 1))
    attns_all = None
    if return_attn and attns_collect:
        # attns_collect: list of arrays (n_layers, batch, seq, seq) -> concatenate along batch
        attns_all = np.concatenate([a for a in attns_collect], axis=1)  # (n_layers, total_batch, seq, seq)
    return preds, ytrue, attns_all

def denorm_flat(arr, scaler_y):
    arr2 = np.array(arr).reshape(-1, 1)
    return scaler_y.inverse_transform(arr2).flatten()

def compute_metrics(y_true, y_pred):
    # shapes (n_samples, horizon)
    rmse = math.sqrt(mean_squared_error(y_true.flatten(), y_pred.flatten()))
    mae = mean_absolute_error(y_true.flatten(), y_pred.flatten())
    # directional accuracy per time-step (ignore first step because diff reduces length)
    if y_true.shape[1] >= 2:
        tdiff = np.sign(y_true[:, 1:] - y_true[:, :-1])
        pdiff = np.sign(y_pred[:, 1:] - y_pred[:, :-1])
        dir_acc = float((tdiff == pdiff).mean())
    else:
        dir_acc = float('nan')
    return {"RMSE": rmse, "MAE": mae, "DirAcc": dir_acc}

# -----------------------
# 5) Baseline: SARIMA (if available) or seasonal-naive fallback
# -----------------------
def sarima_windows(train_series: pd.Series, test_series: pd.Series, horizon: int):
    if not STATSMODELS_AVAILABLE:
        return None
    order = (1, 1, 1)
    seasonal = (1, 1, 1, 24)
    model = sm.tsa.statespace.SARIMAX(train_series, order=order, seasonal_order=seasonal,
                                      enforce_stationarity=False, enforce_invertibility=False)
    res = model.fit(disp=False)
    # forecast full test horizon points (rolling windows)
    preds = res.predict(start=len(train_series), end=len(train_series) + len(test_series) - 1)
    windows = []
    for i in range(0, max(0, len(preds) - horizon + 1)):
        windows.append(preds[i:i+horizon].values)
    if len(windows) == 0:
        return None
    return np.vstack(windows)

def seasonal_naive_windows(train_series: np.ndarray, test_series: np.ndarray, horizon: int, season=24):
    """
    Seasonal-naive: for each forecast window, use the last season-length value(s) as forecast.
    Simpler: for horizon H, repeat last 'season' slice shifted appropriately.
    """
    vals = np.concatenate([train_series, test_series])
    windows = []
    total_len = len(vals)
    start_idx = len(train_series)
    for i in range(0, max(0, len(test_series) - horizon + 1)):
        # naive: for horizon j, use vals[start_idx + i - season : start_idx + i - season + horizon]
        src_start = max(0, start_idx + i - season)
        block = vals[src_start:src_start + horizon]
        if len(block) < horizon:
            # pad with last known
            block = np.pad(block, (0, horizon - len(block)), 'edge')
        windows.append(block)
    if len(windows) == 0:
        return None
    return np.vstack(windows)

# -----------------------
# 6) Run everything
# -----------------------
def run_pipeline():
    # generate data
    df = generate_multivariate_time_series(n_steps=1800, n_features=6)
    train_arr, val_arr, test_arr, scaler_x, scaler_y, df_train, df_val, df_test = split_scale(df, cfg)
    print("Train/Val/Test rows:", train_arr.shape[0], val_arr.shape[0], test_arr.shape[0])

    train_ds = SlidingWindowDataset(train_arr, cfg.input_len, cfg.output_len)
    val_ds = SlidingWindowDataset(val_arr, cfg.input_len, cfg.output_len)
    test_ds = SlidingWindowDataset(test_arr, cfg.input_len, cfg.output_len)

    bs = 64
    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True, drop_last=True)
    val_loader = DataLoader(val_ds, batch_size=bs, shuffle=False)
    test_loader = DataLoader(test_ds, batch_size=bs, shuffle=False)

    input_dim = train_arr.shape[1] - 1  # features count
    # instantiate models
    transformer = SimpleTransformerForecast(input_dim, d_model=96, n_heads=4, n_layers=3,
                                           input_len=cfg.input_len, output_len=cfg.output_len).to(DEVICE)
    lstm = LSTMForecast(input_dim, hidden=128, num_layers=2, output_len=cfg.output_len).to(DEVICE)

    criterion = nn.MSELoss()
    opt_tr = torch.optim.Adam(transformer.parameters(), lr=1e-3)
    opt_lstm = torch.optim.Adam(lstm.parameters(), lr=1e-3)

    # train transformer
    best_val = float('inf'); patience = 6; best_state = None; epochs = 40; best_epoch = -1
    print("Training Transformer...")
    for ep in range(1, epochs + 1):
        _ = train_epoch(transformer, train_loader, opt_tr, criterion, DEVICE)
        preds_val, y_val, _ = evaluate(transformer, val_loader, criterion, DEVICE, return_attn=False)
        val_loss = mean_squared_error(y_val.flatten(), preds_val.flatten())
        print(f"Transformer Epoch {ep} val_mse={val_loss:.6f}")
        if val_loss < best_val - 1e-9:
            best_val = val_loss; best_state = {k: v.cpu() for k, v in transformer.state_dict().items()}; best_epoch = ep
        if ep - best_epoch >= patience:
            print("Transformer early stopping")
            break
    if best_state is not None:
        transformer.load_state_dict(best_state)

    # train LSTM
    best_val = float('inf'); best_state = None; best_epoch = -1
    print("Training LSTM...")
    for ep in range(1, epochs + 1):
        _ = train_epoch(lstm, train_loader, opt_lstm, criterion, DEVICE)
        preds_val, y_val, _ = evaluate(lstm, val_loader, criterion, DEVICE, return_attn=False)
        val_loss = mean_squared_error(y_val.flatten(), preds_val.flatten())
        print(f"LSTM Epoch {ep} val_mse={val_loss:.6f}")
        if val_loss < best_val - 1e-9:
            best_val = val_loss; best_state = {k: v.cpu() for k, v in lstm.state_dict().items()}; best_epoch = ep
        if ep - best_epoch >= patience:
            print("LSTM early stopping")
            break
    if best_state is not None:
        lstm.load_state_dict(best_state)

    # evaluate on test (Transformer with attention)
    print("Evaluating on test set...")
    preds_tr, y_tr, attns = evaluate(transformer, test_loader, criterion, DEVICE, return_attn=True)
    preds_l, y_l, _ = evaluate(lstm, test_loader, criterion, DEVICE, return_attn=False)

    # denormalize
    preds_tr_den = denorm_flat(preds_tr, scaler_y).reshape(-1, cfg.output_len)
    y_tr_den = denorm_flat(y_tr, scaler_y).reshape(-1, cfg.output_len)
    preds_l_den = denorm_flat(preds_l, scaler_y).reshape(-1, cfg.output_len)
    y_l_den = denorm_flat(y_l, scaler_y).reshape(-1, cfg.output_len)

    metrics_tr = compute_metrics(y_tr_den, preds_tr_den)
    metrics_l = compute_metrics(y_l_den, preds_l_den)
    print("Transformer metrics:", metrics_tr)
    print("LSTM metrics:", metrics_l)

    # SARIMA or fallback
    sarima_res = None
    if STATSMODELS_AVAILABLE:
        print("Fitting SARIMA baseline on univariate series (may take a bit)...")
        sarima_preds_windows = sarima_windows(pd.concat([df_train, df_val])["target"], df_test["target"], cfg.output_len)
        if sarima_preds_windows is not None:
            sarima_metrics = compute_metrics(np.vstack([df_test["target"].values[i:i+cfg.output_len]
                                                       for i in range(0, len(df_test) - cfg.output_len + 1)]),
                                            sarima_preds_windows)
            print("SARIMA metrics (approx):", sarima_metrics)
            sarima_res = sarima_preds_windows
    else:
        print("statsmodels not available -> using seasonal-naive baseline")
        naive_windows = seasonal_naive_windows(pd.concat([df_train, df_val])["target"].values,
                                              df_test["target"].values, cfg.output_len, season=24)
        if naive_windows is not None:
            true_windows = np.vstack([df_test["target"].values[i:i+cfg.output_len]
                                      for i in range(0, len(df_test) - cfg.output_len + 1)])
            naive_metrics = compute_metrics(true_windows, naive_windows)
            print("Seasonal-naive metrics (approx):", naive_metrics)

    # Save artifacts
    os.makedirs("outputs", exist_ok=True)
    torch.save(transformer.state_dict(), "outputs/transformer.pth")
    torch.save(lstm.state_dict(), "outputs/lstm.pth")

    # attention visualization: attns shape (n_layers, total_batch, seq, seq)
    if attns is not None:
        # average across batches to get (n_layers, seq, seq)
        avg_per_layer = attns.mean(axis=1)  # axis 1 was batch dimension after concatenate hack
        n_layers = avg_per_layer.shape[0]
        for li in range(n_layers):
            plt.figure(figsize=(6,5))
            sns.heatmap(avg_per_layer[li], cmap="viridis")
            plt.title(f"Layer {li} average self-attention")
            plt.xlabel("key (input time step)")
            plt.ylabel("query (input time step)")
            fname = f"outputs/avg_attn_layer_{li}.png"
            plt.savefig(fname, bbox_inches="tight")
            plt.close()

    # sample predictions plot
    def plot_sample(true_windows, pred_windows, filename, title):
        plt.figure(figsize=(8,4))
        t = np.arange(true_windows.shape[1])
        plt.plot(t, true_windows[0], label="True", linewidth=2)
        plt.plot(t, pred_windows[0], '--', label="Pred")
        plt.title(title)
        plt.xlabel("Horizon step")
        plt.legend()
        plt.savefig(filename, bbox_inches="tight")
        plt.close()

    plot_sample(y_tr_den, preds_tr_den, "outputs/transformer_sample.png", "Transformer sample forecast")
    plot_sample(y_l_den, preds_l_den, "outputs/lstm_sample.png", "LSTM sample forecast")

    # metrics csv
    df_metrics = pd.DataFrame([
        {"model": "transformer", **metrics_tr},
        {"model": "lstm", **metrics_l}
    ])
    df_metrics.to_csv("outputs/metrics.csv", index=False)
    print("Saved outputs to ./outputs/")

if __name__ == "__main__":
    run_pipeline()